from flask import Flask, request, jsonify
import faiss
import pickle
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader 
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.llms.huggingface_pipeline import HuggingFacePipeline
import torch
import re
import json
import logging
import glob

from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline

app = Flask(__name__)

# æ¨¡å‹è·¯å¾‘
model_path = "DeepSeek-R1-Distill-Qwen-14B"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
)

print("ğŸš€ è¼‰å…¥æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# è¼‰å…¥å‘é‡ç´¢å¼• + æ–‡ä»¶æ®µè½
print("ğŸ“š è¼‰å…¥çŸ¥è­˜åº«...")
index = faiss.read_index("vector.index")
with open("docs.pkl", "rb") as f:
    docs = pickle.load(f)

retriever = SentenceTransformer("all-MiniLM-L6-v2")

def retrieve(query, k=3):
    q_embed = retriever.encode([query])
    D, I = index.search(q_embed, k)
    return [docs[i] for i in I[0]]

print("âœ… æº–å‚™å®Œæˆï¼Œé–‹å§‹å•æ›¸ğŸ“˜ï¼è¼¸å…¥ exit å¯é›¢é–‹")

def generate_report(question: str) -> str:
    context_docs = retrieve(question, k=3)
    context = "\n".join(context_docs)


    '''full_prompt = f"""<|user|>
    æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œï¼š
    {context}
    å•é¡Œï¼š{question}
    <|assistant|> (ç¹é«”ä¸­æ–‡)(æ ¹æ“šæª”æ¡ˆçš„æ ¼å¼ç”¢å‡ºå ±å‘Š)(lung-rads-assessment-categories.pdfçš„è³‡æ–™å„ªå…ˆ)(400å­—ä»¥å…§)
    """
'''
    full_prompt = f"""
    è«‹ä½ æ‰®æ¼”ä¸€ä½æ”¾å°„ç§‘é†«å¸«ï¼Œæ ¹æ“šä½ å­¸ç¿’çš„è³‡æ–™ï¼ˆåŒ…å« mydoc.pdf çš„æ•™æå…§å®¹ï¼‰èˆ‡ Lung-RADS è©•ä¼°æŒ‡å¼•ï¼Œæ ¹æ“šä»¥ä¸‹è§€å¯Ÿå…§å®¹æ’°å¯«è¨ºæ–·å ±å‘Šä¸¦ä»¥æ‚£è€…çš„è§’åº¦çµ¦äºˆæ˜“æ‡‚çš„å»ºè­°ã€‚

    è«‹ä¸€å®šè¦ä¾ç…§ä»¥ä¸‹æ ¼å¼ç”¢å‡ºï¼š

    ---
    ä¸€å®šè¦æŒ‰ç…§ä»¥ä¸‹ç”Ÿæˆæ¯ä¸€å€‹ä¸€æ¨¡ä¸€æ¨£çš„é …ç›®å…§å®¹ã€çµæ§‹åŒ–å¡«è¡¨æ¬„ä½ï¼ˆfor formï¼‰ã€‘ï¼š
    è«‹ç”¨ JSON æ ¼å¼å›å‚³ä»¥ä¸‹æ¬„ä½å€¼ï¼ˆå¦‚ç„¡æ³•ç¢ºå®šï¼Œè«‹å¡« nullï¼‰ï¼š

    ```json
    {{
        "tumor_location": "",
        "tumor_size_cm": "",
        "T_stage": "",
        "N_stage": "" ,
        "M_stage": "" ,
        "lung_rads_category": "",
        "other_findings": "" ,
        "imp": {{
            "T": "",
            "N": "",
            "M": ""
        }}
    }}

    ä»¥ä¸‹ç‚ºè§€å¯Ÿå…§å®¹æè¿°ï¼š
    "{question}"
    """

    # Step 2: ç·¨ç¢¼ prompt
    inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    input_ids = inputs["input_ids"]
    input_len = input_ids.shape[1]  # æ‹¿ä¾†å¾Œé¢åˆ‡åˆ†ç”¨

    with torch.no_grad():
        output = model.generate(
            input_ids=input_ids,
            attention_mask=inputs["attention_mask"],
            max_new_tokens=500,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Step 4: å»æ‰ promptï¼Œåªå–æ¨¡å‹æ–°ç”Ÿæˆçš„å…§å®¹
    generated_ids = output[:, input_len:]
    answer = tokenizer.decode(generated_ids[0], skip_special_tokens=True)


    '''inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1000,
            temperature=0.7,
            top_k=50,
            top_p=0.9,
            do_sample=True
        )
    answer = tokenizer.decode(output[0], skip_special_tokens=True).split("</think>\n")[-1]'''

    # æ“·å– JSON å€æ®µ
    json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", answer, re.DOTALL)
    if not json_match:
        json_match = re.search(r"(\{[\s\S]*?\})", answer)

    form_data = None
    if json_match:
        try:
            json_text = json_match.group(1).strip()
            form_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            logging.warning(f"[JSONDecodeError] Failed to parse JSON: {e}")
            form_data = None

    return {
        "report_text": answer,
        "form_data": form_data
    }

@app.route("/generate", methods=["POST"])
def handle_generate():
    data = request.get_json()
    obs = data.get("observation", "")


    if not obs:
        return jsonify({"error": "ç¼ºå°‘ observation æ¬„ä½"}), 400

    result = generate_report(obs)
    return jsonify({"report": result})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8000)